{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The count vectorizer from SKLearn will be used to count all of the words in our Shakespeare corpus."
      ],
      "metadata": {
        "id": "osPq-Dr9dXeS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8QxBKAxoP-Y-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0827db0-1c20-45e9-ba06-3543a68efff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in corpus: 349202\n",
            "Count of 'when': 2225\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import requests\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "response = requests.get(url).text[785:-18093].lower()\n",
        "\n",
        "sentences = nltk.sent_tokenize(response)\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "counts = vectorizer.fit_transform(sentences)\n",
        "word_list = vectorizer.get_feature_names_out().tolist()\n",
        "total_word_count = len(word_list)\n",
        "all_word_counts = np.sum(counts, axis=0).flatten()\n",
        "\n",
        "def ngram_count(ngram: str) -> int:\n",
        "  if ngram not in word_list:\n",
        "    return None\n",
        "\n",
        "  idx = word_list.index(ngram)\n",
        "  count = all_word_counts[0, idx]\n",
        "  return count\n",
        "\n",
        "print(f\"Unique words in corpus: {total_word_count}\")\n",
        "ngram = \"when\"\n",
        "print(f\"Count of '{ngram}': {ngram_count(ngram)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unigram language model always returns the most common word in the corpus."
      ],
      "metadata": {
        "id": "0RjXaBsudfvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_text_gen(in_string: str) -> str:\n",
        "  final_word = word_list[np.argmax(all_word_counts)]\n",
        "  return final_word\n",
        "\n",
        "print(f\"My dog is very {unigram_text_gen('My dog is very')}\")"
      ],
      "metadata": {
        "id": "csYK3sOKWIdG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2053e0-2dda-44c1-8dc7-8fc1738881b8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My dog is very the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigram language model selects the word that most commonly follows the last word of its input."
      ],
      "metadata": {
        "id": "q7aRMFvIdn4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_text_gen(in_string: str) -> str:\n",
        "  last_word = in_string.split(' ')[-1]\n",
        "  candidate_list = []\n",
        "  candidate_count = []\n",
        "  for ngram in word_list:\n",
        "    ngram_list = ngram.split(' ')\n",
        "    if len(ngram_list) != 2:\n",
        "      continue\n",
        "    if ngram_list[0] == last_word:\n",
        "      candidate_list.append(ngram_list[1])\n",
        "      candidate_count.append(ngram_count(ngram_list[1]))\n",
        "  # Handle empty candidate_count\n",
        "  if not candidate_count:\n",
        "    # Return the most common unigram as a fallback\n",
        "    return word_list[np.argmax(all_word_counts)]\n",
        "  else:\n",
        "    return candidate_list[np.argmax(candidate_count)]\n",
        "\n",
        "\n",
        "print(f\"My dog is very {bigram_text_gen('My dog is very')}\")"
      ],
      "metadata": {
        "id": "O6gQrbbAaCEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "395195dd-252f-4151-b0f0-3a35e2f6bc26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My dog is very her\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the implementation of the perplexity measure to compare these two models."
      ],
      "metadata": {
        "id": "gKJQdn2bdwM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(sentence: str) -> float:\n",
        "  words = nltk.word_tokenize(sentence.lower())\n",
        "  # If no words, define perplexity as 1.0 (arbitrary choice for empty input)\n",
        "  if not words:\n",
        "    return 1.0\n",
        "\n",
        "  sum_log_prob = 0.0\n",
        "  N = len(words)\n",
        "\n",
        "  for i in range(N):\n",
        "    # First word uses unigram probability\n",
        "    if i == 0:\n",
        "      count_unigram = ngram_count(words[i])\n",
        "      if count_unigram is None or count_unigram == 0:\n",
        "        return float('inf')\n",
        "      prob = count_unigram / np.sum(all_word_counts)\n",
        "\n",
        "    # Subsequent words use bigram probability\n",
        "    else:\n",
        "      bigram = words[i-1] + \" \" + words[i]\n",
        "      count_bigram = ngram_count(bigram)\n",
        "      count_prev_unigram = ngram_count(words[i-1])\n",
        "\n",
        "      if (count_bigram is None or count_bigram == 0 or\n",
        "          count_prev_unigram is None or count_prev_unigram == 0):\n",
        "        return float('inf')\n",
        "\n",
        "      prob = count_bigram / count_prev_unigram\n",
        "\n",
        "    # Accumulate log probabilities\n",
        "    sum_log_prob += np.log(prob)\n",
        "\n",
        "  # Perplexity = exp(- (1/N) * sum(log(probabilities)))\n",
        "  return float(np.exp(-sum_log_prob / N))"
      ],
      "metadata": {
        "id": "Jx0Ba9XLeYq8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"my dog is very\",\n",
        "    \"when i was young\",\n",
        "    \"the quick brown fox jumps\",\n",
        "    \"it was the best of times\",\n",
        "    \"this is nonsense dzzz\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "  a_output = unigram_text_gen(sentence)\n",
        "  print(f\"Model A says: {sentence} {a_output}\")\n",
        "  print(f\"Perplexity of model A: {perplexity(f'{sentence} {a_output}')}\")\n",
        "\n",
        "  b_output = bigram_text_gen(sentence)\n",
        "  print(f\"Model B says: {sentence} {b_output}\")\n",
        "  print(f\"Perplexity of model B: {perplexity(f'{sentence} {b_output}')}\")"
      ],
      "metadata": {
        "id": "7P5NXDHvefSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe4bc18-3e5e-4524-ab7d-69991aa8a86f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model A says: my dog is very the\n",
            "Perplexity of model A: inf\n",
            "Model B says: my dog is very her\n",
            "Perplexity of model B: 245.38653309406996\n",
            "Model A says: when i was young the\n",
            "Perplexity of model A: inf\n",
            "Model B says: when i was young and\n",
            "Perplexity of model B: inf\n",
            "Model A says: the quick brown fox jumps the\n",
            "Perplexity of model A: inf\n",
            "Model B says: the quick brown fox jumps with\n",
            "Perplexity of model B: inf\n",
            "Model A says: it was the best of times the\n",
            "Perplexity of model A: 95.98762248951924\n",
            "Model B says: it was the best of times the\n",
            "Perplexity of model B: 95.98762248951924\n",
            "Model A says: this is nonsense dzzz the\n",
            "Perplexity of model A: inf\n",
            "Model B says: this is nonsense dzzz the\n",
            "Perplexity of model B: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use this text block to explain the perplexity of the two models. What does\n",
        "this number tell us about the entroy or enthalpy of the models?**"
      ],
      "metadata": {
        "id": "NgK5ZXHzffjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A common metric for evaluating how well a language model (such as a bigram or unigram model) fits or predicts a particular sentence is called perplexity; a higher perplexity suggests greater surprise from the text, whereas a lower perplexity indicates less surprise. Perplexity increases to an endless level when a model gives no probability to any portion of a sentence (for example, an unseen word). Perplexity has nothing to do with enthalpy, a thermodynamic concept that is not relevant to language modeling, but it is strongly related to entropy in information theory, which quantifies uncertainty."
      ],
      "metadata": {
        "id": "-JcRWbrwPGgb"
      }
    }
  ]
}