{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy8j8OUdtP5WNHxllwJ+2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azoqi/Natural-Language-Processing/blob/main/ClassCoding4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem: we will implement a program to convincingly imitate William Shakespeare. To do this we will develop an n-gram language model.\n",
        "\n",
        "Requirements: write a Python program that counts the occurrences of words in THE COMPLETE WORKS OF WILLIAM SHAKESPEARE. You will need to appropriately tokenize the text using NLTK. Your program should support at least a trigram model (i.e. two words followed by a third), but ideally will be parameterized to accept any value of n. The output of your program should be one sentence generated by the trained model."
      ],
      "metadata": {
        "id": "uzpg0R-_N1NW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UbPsKXcONjS_"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import requests\n",
        "import random\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRJfPn16OEtX",
        "outputId": "48c94c1e-05c9-4f2e-e2e4-e5cb68838894"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build an n-gram model as a dictionary mapping (n-1)-tuples (prefixes) to a dictionary of following words and their counts.\n",
        "def build_ngram_model(tokens, n):\n",
        "    model = {}\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        prefix = tuple(tokens[i:i+n-1])\n",
        "        next_word = tokens[i+n-1]\n",
        "        if prefix not in model:\n",
        "            model[prefix] = {}\n",
        "        model[prefix][next_word] = model[prefix].get(next_word, 0) + 1\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "n93DzC8iOKml"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Given a dictionary mapping items to weights, choose an item at random with probability proportional to its weight.\n",
        "def weighted_choice(choices):\n",
        "    total = sum(choices.values())\n",
        "    r = random.uniform(0, total)\n",
        "    cumulative = 0\n",
        "    for word, weight in choices.items():\n",
        "        cumulative += weight\n",
        "        if r <= cumulative:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "rLuGKo8WOVoa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a sentence using the n-gram model. Starts with a random (n-1)-gram (preferring one that starts with a capital letter)\n",
        "# and continues until a terminal punctuation is reached or max_words is exceeded.\n",
        "def generate_sentence(model, n, max_words=50):\n",
        "    # Find starting prefixes that begin with a capital letter.\n",
        "    starters = [prefix for prefix in model.keys() if prefix[0][0].isupper()]\n",
        "    if not starters:\n",
        "        starters = list(model.keys())\n",
        "    current_prefix = random.choice(starters)\n",
        "    sentence = list(current_prefix)\n",
        "\n",
        "    for _ in range(max_words):\n",
        "        if current_prefix not in model:\n",
        "            break\n",
        "        next_words = model[current_prefix]\n",
        "        next_word = weighted_choice(next_words)\n",
        "        if not next_word:\n",
        "            break\n",
        "        sentence.append(next_word)\n",
        "        current_prefix = tuple(sentence[-(n-1):])\n",
        "        # Stop if the new word ends with typical sentence-ending punctuation.\n",
        "        if next_word in ['.', '!', '?'] or next_word.endswith(('.', '!', '?')):\n",
        "            break\n",
        "    return \" \".join(sentence)"
      ],
      "metadata": {
        "id": "ef8OxD0mOfxA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_gutenberg_text(text):\n",
        "    start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "    start_idx = text.find(start_marker)\n",
        "    if start_idx != -1:\n",
        "        text = text[start_idx:]\n",
        "    end_idx = text.find(end_marker)\n",
        "    if end_idx != -1:\n",
        "        text = text[:end_idx]\n",
        "    return text"
      ],
      "metadata": {
        "id": "YJMxK2JgOuTI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    n = 3\n",
        "    if len(sys.argv) > 1:\n",
        "        try:\n",
        "            n = int(sys.argv[1])\n",
        "        except ValueError:\n",
        "            print(\"Invalid value for n; using default n=3.\")\n",
        "\n",
        "    url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
        "\n",
        "    print(\"Downloading Shakespeare text...\")\n",
        "    response = requests.get(url)\n",
        "    text = response.text\n",
        "    text = clean_gutenberg_text(text)\n",
        "\n",
        "    print(\"Tokenizing text using NLTK...\")\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    print(f\"Building a {n}-gram model...\")\n",
        "    model = build_ngram_model(tokens, n)\n",
        "\n",
        "    print(\"\\nGenerating a sentence using the trained model:\")\n",
        "    sentence = generate_sentence(model, n)\n",
        "    print(sentence)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT-0S-IiO0GZ",
        "outputId": "2748030c-c7c1-4d51-f8d8-61cf3f87a2b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid value for n; using default n=3.\n",
            "Downloading Shakespeare text...\n",
            "Tokenizing text using NLTK...\n",
            "Building a 3-gram model...\n",
            "\n",
            "Generating a sentence using the trained model:\n",
            "Morton ; Tell him my most sovereign reason , reason and sanity could not stir from this reproach .\n"
          ]
        }
      ]
    }
  ]
}