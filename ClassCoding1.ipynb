{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azoqi/Natural-Language-Processing/blob/main/ClassCoding1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem: we would like to search Wikipedia for any mention of St. Edward's University. This would include any mentions of St. Ed, St. Ed's, St. Edwards, St. Edward's, St. Edwards' etc.\n",
        "\n",
        "Requirements: write a Python program using the Beautiful Soup module to parse the wiki which can be found at https://en.wikipedia.org/wiki/Main_PageLinks to an external site.. Devise a regular expression to match the St. Ed's pattern described in the problem statement. Your program should print the title of every article that matches the pattern and its URL.\n",
        "\n",
        "You should use a time.sleep function call in your program to avoid making excessive requests to Wikipedia."
      ],
      "metadata": {
        "id": "cWHP9NNWbBYA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdcBrzEJTE9D"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import time\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KlbzItYTE9E"
      },
      "outputs": [],
      "source": [
        "base_url = \"https://en.wikipedia.org\"\n",
        "main_url = base_url + \"/wiki/Main_Page\"\n",
        "\n",
        "# Request the main page\n",
        "response = requests.get(main_url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# Find all links that are likely article links\n",
        "article_links = set()\n",
        "for link in soup.find_all(\"a\", href=True):\n",
        "    href = link[\"href\"]\n",
        "    if href.startswith(\"/wiki/\") and \":\" not in href:\n",
        "        article_links.add(href)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular expression to match any variant of \"St. Edward's\" or \"St. Ed\" etc.\n",
        "# This will match:\n",
        "#    \"St. Ed\", \"St. Ed's\", \"St. Edwards\", \"St. Edward's\", and \"St. Edwards'\"\n",
        "pattern = re.compile(r\"St\\.?\\s+Ed(?:ward(?:'s|s|s')|'?s)?\", re.IGNORECASE)\n",
        "\n",
        "print(\"Searching for articles mentioning St. Edward's University...\\n\")\n",
        "\n",
        "# Loop over each article link\n",
        "for href in article_links:\n",
        "    article_url = base_url + href\n",
        "    try:\n",
        "        article_resp = requests.get(article_url)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {article_url}: {e}\")\n",
        "        continue\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "    article_soup = BeautifulSoup(article_resp.text, 'html.parser')\n",
        "    article_text = article_soup.get_text()\n",
        "\n",
        "    # Search for our pattern in the article text\n",
        "    if pattern.search(article_text):\n",
        "        heading = article_soup.find(\"h1\", id=\"firstHeading\")\n",
        "        title = heading.text.strip() if heading else \"No Title\"\n",
        "        print(f\"Title: {title}\\nURL: {article_url}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "t_6gwyoHTjcM",
        "outputId": "80a5a520-79f7-427d-c763-082a751d2bc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching for articles mentioning St. Edward's University...\n",
            "\n",
            "Title: February 10\n",
            "URL: https://en.wikipedia.org/wiki/February_10\n",
            "\n",
            "Title: Helene Kröller-Müller\n",
            "URL: https://en.wikipedia.org/wiki/Helene_Kr%C3%B6ller-M%C3%BCller\n",
            "\n",
            "Title: Humphrey III of Toron\n",
            "URL: https://en.wikipedia.org/wiki/Humphrey_III_of_Toron\n",
            "\n",
            "Title: 2025 Southeast Europe retail boycotts\n",
            "URL: https://en.wikipedia.org/wiki/2025_Southeast_Europe_retail_boycotts\n",
            "\n",
            "Title: Maria Einsmann\n",
            "URL: https://en.wikipedia.org/wiki/Maria_Einsmann\n",
            "\n",
            "Title: Hottingen (Zurich)\n",
            "URL: https://en.wikipedia.org/wiki/Hottingen_(Zurich)\n",
            "\n",
            "Title: Robbery\n",
            "URL: https://en.wikipedia.org/wiki/Robbery\n",
            "\n",
            "Title: American football\n",
            "URL: https://en.wikipedia.org/wiki/American_football\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0420b182f55e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0marticle_soup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle_resp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}